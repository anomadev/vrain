## Definición 
Un Large Language Model (LLM) es una instancia especializada en un Foundation Model diseñada para procesar y generar secuencias de texto o código. Técnicamente, es una red neuronal masiva basada en la arquitectura Transformer, entrenada mediante aprendizaje auto-supervisado en petabytes de datos para predecir estadísticamente el siguiente elemento (token) en una secuencia, permitiendo una compresión contextual profunda.

## Conceptos Clave
- **Foundation Models**: Modelos base pre-entrenados en datos no etiquetados que sirven como punto de partida para múltiples tareas adaptables.
- **Parámetros**: Son los pasos internos que el modelos ajusta durante el entrenamiento. Representan la "complejidad" y capacidad de almacenamiento de patrones del modelo (ej. GPT-3 tiene 175 mil millones).
- **Arquitectura Transformer**: En núcleo del sistema. Utiliza mecanismos de atención para entender la relación entre palabras sin importar su distancia en el texto, construyendo un mapa semántico del contexto.
- **Next Token Prediction**: El objetivo de entrenamiento. El modelo aprende a minimizar la diferencia entre su predicción y el dato real, pasando de "adivinanzas aleatorias" a coherencia gramatical y lógica.
- **Fine-tuning**: El proceso de especializar un modelo general en un set de datos pequeño y específico para tareas de nicho o dominios de negocio particulares.

## Notas Personales
El mayor reto al aprender sobre LLMs es dejar de verlos como bases de datos. No "almacenan" información, sino "patrones de lenguaje". Esto explica por qué pueden alucinar: están optimizados para la verosimilitud (que suene real), no necesariamente para la veracidad factual. Para un arquitecto, la clave está en el Context Window y cómo alimentamos al modelo con la data correcta (RAG) para que sus predicciones se bases en nuestros hechos.
