## Definición
La terminología común en IA constituye el lenguaje técnico necesario para describir la arquitectura, el comportamiento y las limitaciones de los sistemas basados en modelos de lenguaje. Para un Ingeniero de Software, estos términos definen las métricas de rendimiento (throughput), las restricciones de memoria (context window) y las vulnerabilidades de seguridad (injection) de la aplicación.

## Conceptos Clave
- **LLM**: Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados con grandes datos de texto para comprender y generar un lenguaje similar al humano. Funcionan como motores de predicción, analizando la información y prediciendo el siguiente token más probable. Los LLM realizan tareas como generación de texto, traducción, resumen y preguntas y respuestas. Comprender el procesamiento de tokens es clave para una ingeniería rápida y eficaz.
- **Tokens**: La unidad atómica de procesamiento. No son palabras exactas, sino fragmentos (1k tokens $\approx$ 750 palabras). El costo y la velocidad de tu API se miden aquí. Comprender los tokens es crucial porque los modelos predicen el próximo token en las secuencias, los costos de la API se basan en el recuento de tokens y los modelos tienen límites máximos de tokens para la entrada y la salida.
- **Context Window**: La "memoria RAM" de corto plazo del modelo. Es el límite máximo de tokens (entrada + salida) que el modelo puede procesar de una sola vez. Al excederse, las partes más antiguas se truncan. Comprender esta restricción es crucial para prompt engineering: es necesario encontrar un equilibrio entre proporcionar suficiente contexto y mantenerse dentro de los límites de tokens.
- **Hallucination (Alucinación)**: Cuando el modelo genera información factualmente incorrecta pero gramaticalmente coherente debido a su naturaleza probabilística. Esto ocurre cuando los modelos llenan vacíos de conocimiento o presentan información incierta con aparente certeza. Las técnicas de mitigación incluyen solicitar fuentes, pedir niveles de confianza, proporcionar contexto y verificar siempre la información crítica de forma independiente.
- **Agents (Agentes)**: Sistema que usan un LLM como "cerebro" para tomar decisiones, usar herramientas (APIs, DBs) y ejecutar tareas de forma autónoma para alcanzar un objetivo. Combinan la comprensión del lenguaje con el uso de herramientas, la memoria y la toma de decisiones para realizar tareas complejas de varios pasos. 
- **Prompt Injection**: Una vulnerabilidad de seguridad donde un usuario "engaña" al modelo para que ignore sus instrucciones originales y ejecute comandos maliciosos. Los atacantes incrustan instrucciones en los datos para que los modelos ignoren las indicaciones originales y sigan comandos maliciosos. La mitigación requiere la limpieza de la entrada, un diseño de indicaciones resistente a la inyección y limites de seguridad adecuados.
- **Model Weights / Parameters**: Los valores numéricos aprendidos durante el entrenamiento que determinan cómo se transforma el input en output. Definen el "conocimiento" del modelo. Los parámetros son las variables entrenables ajustadas durante el entrenamiento, mientras que los pesos representan sus valores finales.
- **Fine-tuning vs Prompt Engineering**: El fine-tuning entrena modelos en datos específicos para especializar el comportamiento, mientras que el prompt engineering([[5.1.3 What is Prompt Engineering]]) logra la personalización a través de diseño de entrada si modificación del modelo. El prompt engineering es más rápido, económico y accesible. El fine-tuning ofrece una mayor personalización, pero requiere una cantidad considerable de recursos y experiencia.
- **RAG (Retrieval-Augmented Generation)**: Técnica que recupera datos externos (ej. tus leyes de Comercio Exterior) y los inyecta en el prompt para que el modelo responda basándose en hechos reales, reduciendo alucinaciones. RAG recupera documentos relevantes antes de generar respuestas, lo que reduce las alucinaciones y permite el acceso a la información más allá del límite de entrenamiento del modelo. Este enfoque mejora la precisión y permite atribuir la fuente.
- **AI vs. AGI**: La IA actual es "estrecha" (específica a tareas). La AGI (Inteligencia Artificial General) es la frontera hipotética donde una máquina iguala o supera la inteligencia humana en cualquier tarea intelectual.
