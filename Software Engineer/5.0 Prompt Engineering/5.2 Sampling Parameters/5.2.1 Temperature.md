## Definición
La temperatura es un hiperparámetro que escala los *logits* (*puntuaciones*) de un modelo para controlar la aleatoriedad de la respuesta. Matemáticamente, influye en la distribución de probabilidad de los tokens: cuando $T \to 0$, el modelo se vuelve determinista, eligiendo siempre el token con mayor probabilidad. Cuando $T > 1$, la distribución se "aplana", dándole oportunidad a tokens menos probables, lo que percibimos como creatividad.

## Conceptos Clave
- **Determinismo ($T \approx 0$)**: El modelo se vuelve predecible. Ideal para tareas técnicas, extracción de datos y generación de código donde solo hay una "respuesta correcta".
- **Creatividad/Variabilidad ($T > 0.7$)**: El modelo se aleja del camino más probable. Útil para brainstorming, redacción de correos o marketing.
- **Softmax Scaling**: Es el proceso matemático donde la temperatura divide los *logits*. Una temperatura baja acentúa las diferencias de probabilidad; una alta las nivela.
- **Riesgo de Alucinación**: A mayor temperatura, mayor es el riesgo de que el modelo genera información factualmente incorrecta o roma el formato (ej. JSON mal formado).

