## Definición 
Max Tokens es un parámetro de configuración que establece un límites estricto al número máximo de `tokens` ([[5.1.5 Common Terminology]]) que el modelo puede generar en una sola respuesta. Es la frontera que separa una respuesta concisa de una generación infinita que agota tu presupuesto y degrada la experiencia de usuario debido a la latencia acumulada.

### Conceptos Clave
- **Response Budgeting**: Es la reserva de espacio dentro de la Ventana de Contexto. Si tu ventana es de 8K y usas 7K en el prompt, `max_tokens` de 2K fallará o causará un error de "`context window exceeded`".
- **Latency Correlation**: En los `LLMs` el tiempo de respuesta es lineal al número de tokens generados. A mayor `max_tokens`, mayor es el tiempo máximo que el usuario podría esperar.
- **Finish Reason**: Cuando el modelo alcanza este límite antes de terminar su idea, el API devuelve un estado específico (usualmente `lenght`). Es un indicador clave para el manejo de errores en el código.
- **Token vs. Completion**: `max tokens` no obliga al modelo a escribir hasta ese límite; simplemente le prohíbe pasar de ahí.

## Escenarios de Aplicación e Identificación
**Dónde aplicarlos?**
- **Micro-copy / UI Elements**: Si diseñas un agente que genera títulos de artículos o notificaciones, un `max_tokens` de 20-50 asegura que el diseño de tu interfaz no se rompa por un texto excesivamente largo.
- **Cost Control**: En entornos de producción con miles de usuarios, limitar la respuesta a 500 tokens en lugar de 4000 puede recudir la factura mensual de API en un factor significado.
- **Data Extraction**: Cuando esperas un JSON o una respuesta de una sola palabra, limitar tokens evita que el modelo divague o agregue explicaciones innecesarias.
**Cómo identificarlo?**
- **Truncamiento Abrupto**: Si la respuesta del modelo termina a mitad de una palabra o de una etiqueta `HTML/JSON`, es el síntoma clásico de un `max_tokens` demasiado bajo.
- **Timeout de Red**: Si tu sistema experimenta `timeouts` consistentes en ciertas peticiones, revisa si el `max_tokens` es muy alto y el modelo está generando una "tesis" no deseada.

## Notas Personales
El error más común es establecer un `max_tokens` arbitrariamente alto (el. 40%) "por si acaso". Esto es peligroso en sistemas distribuidos. Un buen arquitecto siempre define el `max_tokens` basándose en el contrato de la interfaz. Si tu función espera un objeto `User`, no necesitas 2000 `tokens`; con 200 basta.
