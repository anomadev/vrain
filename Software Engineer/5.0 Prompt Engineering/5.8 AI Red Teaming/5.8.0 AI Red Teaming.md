## Definición
El **AI Red Teaming** es una metodología de evaluación de seguridad donde un grupo de "atacantes éticos" (el equipo rojo) intenta deliberadamente comprometer un sistema de IA. El objetivo es identificar debilidades como el **Jailbreaking**, la extracción de datos sensibles (`PII leakage`), sesgos dañinos o vulnerables de **`Prompt Injection`** ([[5.1.2 What is a Prompt]]). A diferencia de las pruebas de QA tradicionales, el **Red Teaming** es adversarial y busca escenarios no previstos por los desarrolladores.

## Conceptos Clave
- `Adversarial Attacks`: Intentos de manipular la entrada del modelo para que produzca un resultado incorrecto o prohibido (*ej. añadir ruido invisible a una imagen o texto*).
- `Jailbreaking`: Técnicas de ingeniería de prompts diseñadas para evadir las restricciones de seguridad (`guardrails`) impuestas por el sistema (*ej. ataques de DAN o Do Anything Now*).
- `Prompt Injection (Direct & Indirect)`: Insertar instrucciones maliciosas en el prompt para subvertir el propósito original del agente. La inyección indirecta ocurre cuando el modelo lee datos externos (como un sitio web) que contiene el ataque.
- `PII Leakege`: Riesgo de que el modelo revele información de identificación personal que formaba parte de su set de entrenamiento o del contexto actual.
- `Guardrails (Barandillas)`: Sistemas de filtrado (programáticos o modelos menores) que actúan como cortafuegos entre el usuario y el LLM ([[5.1.1 LLMs and how they work]]) para detectar ataques.

## Escenarios de Aplicación e Identificación
**¿Dónde aplicar lo?**
- **Auditorías de Pre-Lanzamiento**: Antes de que un agente de IA interactúe con clientes reales para asegurar que no dará consejos financieros o médicos ilegales.
- **Sistemas RAG ([[5.1.5 Common Terminology]])**: Para asegurar que el modelo no acceda a documentos de otros usuarios si se le manipula mediante el chat (Multi-tenancy security)
- **Modelos de clasificación**: Verificar que el modelo no sea engañado por ataques adversariales que cambien una etiqueta de "Malicioso" con una pequeña modificación.
**¿Cómo identificarlo?**
- **Anomalías en el Comportamiento**: Si el modelo empieza a responder en un tono que no le corresponde o ignora las instrucciones del System Prompt ([[5.6.3 System Prompting]]).
- **Fugas de Datos en Logs**: Logs que muestran al modelo intentando ejecutar código o revelar variables de entorno debido a una inyección.

## Nota Personales
En el diseño de sistemas, tendemos a ser optimistas. El AI Red Teaming nos obliga a ser pesimistas por diseño. Un error común es pensar que el "`System Prompt`" es secreto; en Red Teaming, asumimos que el usuario eventualmente podrá leerlo. La clave es la Defensa en Profundidad: no confíes solo en el prompt; implementa validadores de salida programáticos (Regex, clasificadores de toxicidad) y sanitiza todas las entradas de terceros.