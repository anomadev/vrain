## Definición 
El `Prompting Debiasing` es el conjunto de técnicas aplicadas durante el diseño del prompt ([[5.1.2 What is a Prompt]]) para mitigar o eliminar los sesgos algorítmicos heredados por el modelo durante su pre-entrenamiento. Matemáticamente, busca desplazar la distribución de probabilidad $P(y|x)$ hacia un estado de neutralidad, asegurando que la respuesta no varíe injustamente ante cambios en atributos protegidos (como el nombre, género o nacionalidad del individuo).

## Conceptos Clave
- **`Training Data Bias`**: El sesgo que el modelo "aprendió" por defecto debido a que el internet no es un lugar neutral.
- **`Instruction-based Mitigation`**: El uso de directivas explícitas en el `System Prompt` ([[5.6.3 System Prompting]]) que ordenan al modelo ignorar estereotipos y mantener la objetividad.
- **`Few-shot Calibration`**: Proporcionar ejemplos balanceados que muestren explícitamente la neutralidad deseada para "calibrar" la respuesta del modelo ([[5.6.2 One-Shot & Few-Shot Prompting]]).
- **`Counterfactual Testing`**: Una técnica de validación donde cambiamos una variable sensible (*ej. Juan por María*) y verificamos si el output técnico se mantiene idéntico.
- **`Ouput Fairness`**: La garantía de que el resultado final cumple con estándares éticos y legales de no discriminación.

## Escenarios de Aplicación e Identificación
**¿Dónde encontrarlo o aplicarlo?**
- **Sistemas de Reclutamiento Automatizado**: Para asegurar que el análisis de un CV se base en habilidades técnicas y no en el origen o género del candidato.
- **Modelos de Scoring Crediticio**: Para evitar que el sistema penalice a un solicitante basándose en prejuicios demográficos o geográficos.
- **Moderación de Contenido**: Al diseñador filtros que deben ser imparciales y no censurar opiniones basadas en sesgos culturales del modelo.
**¿Cómo identificar la necesidad de debiasing?**
- **Inconsistencias en Pruebas A/B**: Si al cambiar el nombre de una persona en el prompt la recomendación técnica cambia drásticamente.
- **Respuestas Estereotipadas**: El modelo asume roles por defecto (*ej. asume que un "doctor" es hombre o que un "asistente" es mujer*).

## Notas Personales
En ingeniería, solemos decir que "Garbage In, Garbage Out". El sesgo es basura de datos. Un error común de arquitectura es ignorar este tema hasta que el sistema está en producción y causa un problema legal o ético. Considera el Debaising como una forma de Sanitización de Datos. No confíes en la "bondad" del modelo; implementa reglas estrictas en el `System Prompt` para forzar la objetividad.