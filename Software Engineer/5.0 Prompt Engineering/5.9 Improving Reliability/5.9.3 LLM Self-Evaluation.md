## Definición
La Auto-evaluación (`Self-Evaluation`) es una técnica de fiabilidad donde se instruye a un modelo de lenguaje para que actúe como crítico de su propio output o del de otro modelo. El sistema no solo genera una respuesta, sino que ejecuta un paso adicional de introspección para calificar, puntuar o corregir dicha respuesta basándose en criterios específicos (rúbricas) como veracidad, tono, seguridad o cumplimiento de formato.

## Conceptos Clave
- **`Self-Correction`:** El proceso donde el modelo identifica un error en su razonamiento previo y genera una versión corregida.
- **`Evaluator-as-a-Service`:** El patrón de diseño donde un modelo "Juez" (usualmente uno más potente, como GPT-4o o Claude 3.5 Sonnet) evalúa el trabajo de un modelo "Trabajador" (más rápido/barato).
- **`Rubric-based Scoring`:** El uso de una matriz de evaluación explícita en el prompt para que el modelo asigne puntajes numéricos o categóricos (ej. "Del 1 al 5, ¿qué tan técnica es la respuesta?").
- **`Critique-and-Refine`:** Un ciclo iterativo donde el modelo genera, critica sus debilidades y refina el resultado hasta alcanzar un umbral de calidad.

## Escenarios de Aplicación e Identificación
**¿Dónde aplicarlo?**
- **Sistemas de Auditoría de Cumplimiento:** Antes de enviar una respuesta legal o financiera, un agente evaluador verifica que se hayan incluido todos los "disclaimers" obligatorios.
- **Pipeline de Generación de Datos Sintéticos:** Evaluar si los datos generados por una IA son lo suficientemente diversos y realistas antes de usarlos para entrenar otros modelos.
- **Validación de Formatos Complejos:** Verificar que un JSON generado no solo sea válido sintácticamente, sino que los valores tengan sentido lógico dentro del dominio de negocio.
**¿Cómo identificarlo?**
- **Flujos Multietapa:** Identificas esta técnica cuando ves que una sola petición del usuario dispara dos o más llamadas al LLM: una para "Responder" y otra para "Validar".
- **Prompt de Metadatos:** El output final del sistema incluye campos como `eval_score`, `critique` o `is_valid: true/false`.

## Notas Personales
La auto-evaluación es el **"Unit Testing" semántico**. En software tradicional, testeamos que `2+2=4`. Con LLMs ([[5.1.1 LLMs and how they work]]), usamos Self-Eval para testear que "el tono sea profesional". Mi recomendación técnica: **Nunca dejes que el modelo se autoevalúe en el mismo turno (mismo prompt)**. El modelo sufrirá de inercia cognitiva. Siempre separa la evaluación en una llamada de API distinta con un `System Prompt` que le ordene ser un "crítico implacable".