## Definición
La **Calibración** es el grado de correspondencia entre la probabilidad de confianza predicha por un modelo y la precisión real de su respuesta. Un modelo perfectamente calibrado es aquel donde, si afirma tener un 80% de confianza en un conjunto de respuestas, exactamente el 80% de ellas son correctas. En los LLMs ([[5.1.1 LLMs and how they work]]), este proceso es complejo porque tienden a ser **sobreconfiados (overconfident)**, especialmente tras procesos de RLHF (Reinforcement Learning from Human Feedback).

## Conceptos Clave
-  `Overconfidence`: Tendencia de los modelos a asignar probabilidades cercanas al 100% a respuestas que son incorrectas.
- `ECE (Expected Calibration Error)`:  Métrica matemática que mide la diferencia promedio entre la confianza y la precisión. Un ECE de 0 indica una calibración perfecta.
- `Temperature Scaling`: Técnica de post-procesamiento que ajusta los *logits* antes de Softmax para "suavizar" las probabilidades sin cambiar el orden de los resultados.
- `Verbalized Confidence`: Pedirle al modelo que exprese su confianza en lenguaje natural (*ej. Es una escala del 1 al 10, qué tan seguro estás?*)
- `Logit Analysis`: Acceder a las probabilidades crudas de los tokens a través de la API (`logprobs`) para calcular la incertidumbre de forma programática.

## Escenarios de Aplicación e Identificación
**¿Dónde aplicarlo?**
- **Sistemas de Soporte a Decisiones Médicas o Legales**: Donde una respuesta con baja confianza debe ser marcada obligatoriamente para revisión humana.
- **Pipelines de RAG ([[5.1.5 Common Terminology]])**: Para destacar respuestas donde el modelo no encuentra suficiente evidencia en el contexto inyectado.
- **Clasificación Automatizada**: Si la confianza es < 0.8, enviar el ticket a una cola de revisión manual en lugar de procesarlo automáticamente.
**¿Cómo identificarlo?**
- **Alucinaciones con Certeza**: el modelo da un dato falso usando un lenguaje extremadamente asertivo ("La capital de Marte es New York")
- **Respuestas Binarias**: El modelo rara vez usa términos con "probablemente", "tal vez" o "no estoy seguro", a menos que se le fuerce.

## Notas Personales
Como arquitectos, debemos ser escépticos ante la asertividad de la IA. Un LLM es, por definición, un "mentiroso convincente". Mi punto crítico: No confíes en la confianza nativa del modelo. En sistemas de producción, siempre implemento una capa de validación cruzada. Si el modelo dice esta al 99%, pero los `logsprobs` muestran una distribución plana, ignoro su asertividad y trato la respuesta como "No confiable".